{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2925605c-847a-4131-a1f2-244d34811261",
   "metadata": {},
   "source": [
    "Machine Leaning models ðŸ¤–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d277155f",
   "metadata": {
    "tags": []
   },
   "source": [
    "*contact :mehdi.zouitine@irt-saintexupery.com*\n",
    "\n",
    "# Labs 2: Machine Leaning models\n",
    "\n",
    "The last exercises of the session will be devoted to the implementation of more complex models and the use of a two in order to solve a task on a real dataset.\n",
    "\n",
    "\n",
    "\n",
    "The objectives of this lab: **Discover, use and implement from scratch many classical machine learning algorithms !**\n",
    "\n",
    "Note that in machine learning and in informatic in general, when you do not know how to do something or how does something work, your first reflex should be to look on **the documentation** and more generally on internet. First, all the documentation of the libraries you will use is there. Moreover, Python has a huge community (one of its strength) and therefore a lot of trouble you may have has already been resolved on a forume (such as StackOverflow). \n",
    "\n",
    "**However**, you should know that code you found on forum might be under some copyrights it is the case for instance on all the code avaible on StackOverflow\n",
    "\n",
    "To find the documentation on a class, a function or a method from a library, most of the time, searching the name of the library and the name of the method will give you the documentation as the first link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3de973-2f86-4606-b766-04a83968e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol, Tuple, List #Typing library : In python the types of variables\n",
    "# are not declared explicitly.\n",
    "#However, it is sometimes useful to specify them to improve the readability of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9bcad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64af259d-6f9a-4b37-abaa-dc7b8dbedaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eaed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set() # graphical library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55fcdd8-5f20-4d5d-bfd6-02147ed151a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit learn is the reference library in python for using machine learning models (mainly on tabular data)\n",
    "import sklearn\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16972a7-57d3-4155-8f10-911a63437c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# can use named colors or HTML codes\n",
    "colormap = np.array(['red', 'lightseagreen', '#F39C12'])\n",
    "cmap = sns.dark_palette((200, 1000,45), input=\"husl\",as_cmap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c4f5f",
   "metadata": {},
   "source": [
    "## 1.0 Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e2624a",
   "metadata": {},
   "source": [
    "This first exercise has two purposes: \n",
    "\n",
    "- To introduce you to the notion of class with a data science-oriented practical case\n",
    "\n",
    "- To see under the hood the notion of data scaling\n",
    "\n",
    "\n",
    "Many normalization methods are possible, but in 99% of the use cases.\n",
    "you will use the `MinMaxScaler`: $$Z= \\frac{X - \\bar{X}}{\\sigma(X)}$$ or `StandardScaler`: $$S = \\frac{X - \\min(X)}{\\max(X) - \\min(X)}$$.\n",
    "The first set your data between 0 and 1 and the second to mean 0 and standard deviation 1. \n",
    "\n",
    "Why should you scale your data?\n",
    "\n",
    "Scale effects can interfere with the training of many estimators. It is necessary to scale the data where the estimator uses distance or gradient during training.\n",
    "\n",
    "Advice: always normalize your data except if you want to use an algorithm based on decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54b31c9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.0.1 : </b>\n",
    "\n",
    "Implement the two normalization methods using the object formalism.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cdf1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        self.epsilon = 1e-6\n",
    "    \n",
    "    def fit(self, X: np.array, y=None): # y=None is a scikit learn convention\n",
    "        # Compute the mean for each columns\n",
    "        self.mean = ...\n",
    "        # Compute the standard deviation for each columns\n",
    "        self.std = ...\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        X_norm = ... # safe division\n",
    "        return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca29a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxNormalizer:\n",
    "    def __init__(self):\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "        self.epsilon = 1e-6\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # the floor is yours\n",
    "        ...\n",
    "    \n",
    "    def transform(self, X:np.ndarray) -> np.ndarray:\n",
    "        # (X - X_min) / (X_max - X_min) \n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474620fd",
   "metadata": {},
   "source": [
    "## 1.1 Generate the toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e800944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "def eval_model(Y_pred:np.ndarray, Y_true:np.ndarray, classification:bool=True):\n",
    "    metric = accuracy_score if classification else mean_squared_error\n",
    "    score = metric(Y_true, Y_pred)\n",
    "    return f\"The score is {score:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440b169d-4a0e-4dc4-a9b5-129eb0f5c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_dataset(mode: str = \"train\") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    random_state = 42 if mode == \"train\" else 1337\n",
    "    X, y = make_moons(n_samples=1000,random_state=random_state, noise=0.20)\n",
    "    return X, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab11a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regression_dataset(mode: str = \"train\") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    random_state = 42 if mode == \"train\" else 1337\n",
    "    np.random.seed(random_state)\n",
    "    X_1 = np.random.uniform(size = 1000, low = 0, high = 25)\n",
    "    X_2 = np.random.uniform(size = 1000, low = 0, high = 8)\n",
    "    gaussian_noise = np.random.normal(loc=0, scale=4, size=1000)\n",
    "    Y = 2 * X_1 - 3 * X_2 + gaussian_noise\n",
    "    X = np.stack([X_1, X_2], axis =-1)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82eb5cc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.1.1 : </b>\n",
    "\n",
    "Using the previous functions : Generate a train and test dataset for the two types of task (classification and regression)\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb7597-177e-4281-9fb3-19f82d6253a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "X_train_clf, Y_train_clf = generate_classification_dataset(mode=\"train\")\n",
    "X_test_clf, Y_test_clf = generate_classification_dataset(mode=\"test\")\n",
    "\n",
    "X_train_reg, Y_train_reg = generate_regression_dataset(mode=\"train\")\n",
    "X_test_reg, Y_test_reg = generate_regression_dataset(mode=\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cfb7ad",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.1.2 : </b>\n",
    "\n",
    "Use your favorite normalization to scale your data .\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "#X_train_clf_norm = ...\n",
    "#X_test_clf_norm = ...\n",
    "\n",
    "#X_train_reg_norm = ...\n",
    "#X_test_reg_norm = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbcf903-65e6-4224-9d05-f95493fbe6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(X:np.ndarray, Y: np.ndarray, colormap: np.ndarray = colormap,classification=True) -> None:\n",
    "    if classification:\n",
    "        plt.scatter(X[:, 0], X[:, 1], color=colormap[Y],s=10)\n",
    "    else:\n",
    "        \n",
    "        points = plt.scatter(X[:, 0], X[:, 1], c=Y, s=15, cmap=cmap)\n",
    "        plt.colorbar(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d250d65",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.1.3 : </b>\n",
    "\n",
    "Briefly explain this dataset (features and the machine learning task to be performed).\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802dcf51-a879-4d25-8d88-410c1e537862",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(X=X_train_clf, Y=Y_train_clf, colormap=colormap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a785f9b5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.1.4 : </b>\n",
    "\n",
    "Briefly explain the dataset (features and the machine learning task to be performed).\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(X=X_train_reg, Y=Y_train_reg, colormap=colormap,classification=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3451924",
   "metadata": {},
   "source": [
    "## 1.2 General information on machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736ea5d",
   "metadata": {},
   "source": [
    "It is possible to implement a machine learning model in many ways. However, there are **code standards** that allow for readability and understanding by all. It is common to implement a machine learning model with the sklearn framework. We will ask you to implement your methods in the form of a **python class** with at least two methods: the **fit** method and the **predict** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5870f0-65bd-4f6f-9f0a-bfe4177d1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineLearningModel(Protocol): \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, *args, **kwargs):\n",
    "        ...\n",
    "    def predict(X: np.ndarray, *args, **kwargs) -> np.ndarray:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b6f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc # abstract class\n",
    "\n",
    "class AibtMachineLearningModel(abc.ABC):\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> \"AibtMachineLearningModel\":\n",
    "        raise NotImplementedError(\"You should implement fit method\")\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def predict(self, X:np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError(\"You should implement predict method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68452a3a-1943-48e0-a73c-896a62689335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundaries(X: np.ndarray, Y: np.ndarray, classifier: MachineLearningModel):\n",
    "    # Plotting decision regions\n",
    "    f = plt.figure()\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plot_dataset(X=X, Y=Y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6647e2b",
   "metadata": {},
   "source": [
    "A machine learning model is a function  $f_\\theta$ parametrized by a set of parameters $\\theta$.\n",
    "    \n",
    "* Train a model means : Find the best $\\theta$ parameters $\\theta^*$ that minimize a given loss function $\\mathcal{L}$.\n",
    "\n",
    "\n",
    "* The loss function $\\mathcal{L}$ allows to evaluate the prediction error of the model with respect to a ground truth or label $y$ :  $$err(y,\\hat{y}) = \\mathcal{L}(y,f_\\theta(x))$$\n",
    "\n",
    "\n",
    "* These theta parameters can be found iteratively by following the opposite direction of the gradient of the loss function.\n",
    "\\begin{equation}\n",
    "\\theta_{i+1}=\\theta_{i}-\\tau_{i} \\nabla \\mathcal{L}\\left(\\theta_{i}\\right)\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a3fa7",
   "metadata": {},
   "source": [
    "<img src=\"./images/ball.png\" alt=\"tree\" width=\"600\"/>\n",
    "source:rasbt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b673ac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<b> Example : </b>\n",
    "Let's imagine that we want to train a model to predict the life expectancy $y$ according to some biological data $x$ : age, weight, height, blood profile ...\n",
    "We have to find by training the set of parameters $\\theta$ of our model $f_\\theta$ which minimizes the prediction error. This error is defined as the distance between the predicted life expectancy $\\hat{y}$ and the true life expectancy $y$ : \n",
    "    \n",
    "$$err(y,\\hat{y}) = \\mathcal{L}(y,f_\\theta(x))$$ with : $$\\mathcal{L}(y,\\hat{y}) = \\Vert y - \\hat{y} \\Vert_2$$ (MSE loss)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd5ac9d",
   "metadata": {},
   "source": [
    "## 1.3 Machine learning models implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7dae38",
   "metadata": {},
   "source": [
    "### 1.3.1 A first classification model: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b1dcf",
   "metadata": {},
   "source": [
    "The logistic regression model is the basic model in binary classification. It has the advantage of being very simple and easily explained. Despite its simplicity, it is widely used, especially in the banking world.\n",
    "\\begin{equation}\n",
    "S\\left(X^{(i)}\\right)=\\theta_{0}+\\theta_{1} x_{1}+\\theta_{2} x_{2}+\\theta_{3} x_{3}+\\ldots .+\\theta_{n} x_{n} = \\sum_{i=0}^{n+1}\\left(\\theta_{i} x_{i}\\right) = \\Theta X^{(i)}\n",
    "\\end{equation}\n",
    "with $\\theta_i \\in \\mathbf{R}$ for all $i \\in 1,2,\\ldots,n$.\n",
    "\n",
    "$\\Theta$ is the vector of parameters to be estimated and $S$ is called the score function.\n",
    "The idea of the logistic regression is to find coefficients $\\theta_1,\\theta_2,\\ldots,\\theta_n$ such that \n",
    "* $S\\left(X^{(i)}\\right)>0$ when the label of $i$ is $1$.\n",
    "* $S\\left(X^{(i)}\\right)<0$ when the label of $i$ is $0$.\n",
    "\n",
    "However, a classification model should return a probability of belonging to a class and not a score.\n",
    "To go from a score function to a probability we will use the logistic function (or sigmoid) : \n",
    "\n",
    "\\begin{equation}\n",
    "\\operatorname{Sigmoid}(x)= \\sigma(x)=\\frac{1}{1+e^{-x}}\n",
    "\\end{equation}\n",
    "The probability that individual $i$ belongs to class $1$ is therefore modeled by :\n",
    "\n",
    "\\begin{equation}\n",
    "P(y=1 \\mid X^{(i)} , \\Theta) = \\sigma(S\\left(X^{(i)}\\right))\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "P(y=0 \\mid X^{(i)} , \\Theta) = 1-P(y=1 \\mid X^{(i)} , \\Theta) = 1 - \\sigma(S\\left(X^{(i)}\\right))\n",
    "\\end{equation}\n",
    "\n",
    "In practice if $X$ is the sample matrix, $\\Theta$ the parameters vector and $\\sigma$ the sigmoid function.\n",
    "So, the logistic model is defined as :\n",
    "\n",
    "<span style=\"color:red\"> $$M(X,\\Theta) = \\sigma(X \\Theta) $$</span>\n",
    "\n",
    "The best $\\theta$ parameters can be found iteratively by following the opposite direction of the gradient of the loss function defined as follow :\n",
    "\n",
    "\\begin{equation}\n",
    "L_{\\mathrm{CE}}(\\hat{y}, y)=-\\log p(y \\mid x)=-[y \\log \\hat{y}+(1-y) \\log (1-\\hat{y})]\n",
    "\\end{equation}\n",
    "this loss is called the cross-entropy.\n",
    "\n",
    "\\begin{equation}\n",
    "L_{\\mathrm{CE}}(\\hat{y}, y)=-[y \\log \\sigma(\\theta \\cdot x)+(1-y) \\log (1-\\sigma(\\theta \\cdot x))]\n",
    "\\end{equation}\n",
    "\n",
    "<span style=\"color:blue\">\\begin{equation}\n",
    "\\frac{\\partial L_{\\mathrm{CE}}(\\hat{y}, y)}{\\partial \\theta}=[y-\\sigma(\\theta \\cdot x)] x=[y-\\hat{y}] x\n",
    "\\end{equation}</span>\n",
    "\n",
    "**The gradients are noted in a vector manner to facilitate the transition to the implementation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b261971c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b> Note : </b>\n",
    "    In machine learning, understanding the dimension of tensors is essential. This allows to better understand the algorithms. Knowing the dimensions in Machine Learning is a bit like dimensional analysis in physics. If I know the shape of my tensors and the operations I perform then I can understand the output shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849a684",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.1.1 : </b>\n",
    "\n",
    "What is the dimension $X$, $\\Theta$ and the output $\\hat{y}$ of the model ?\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b09f55",
   "metadata": {},
   "source": [
    "#### Sklearn logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c9a7a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.1.2 : </b>\n",
    "\n",
    "1. Train a sklearn logistic regression model on the train data. \n",
    "\n",
    "2. Display the decision boundaries.\n",
    "\n",
    "3. Evaluate the model on the test data using the `eval_model` method defined above.\n",
    "\n",
    "4. Try to get the best score and the most beautiful decision boundaries.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4323a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2fb0ca",
   "metadata": {},
   "source": [
    "#### Homemade logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe33806",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.1.3 : </b>\n",
    "\n",
    "Implement logistic regression from scratch.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e5081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice\n",
    "class Sigmoid:\n",
    "    def __call__(self,x):\n",
    "        return ...\n",
    "class HomemadeLogisticRegression(AibtMachineLearningModel):\n",
    "    \"\"\" Logistic Regression classifier.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate: float\n",
    "        The step length that will be taken when following the negative gradient during\n",
    "        training.\n",
    "    gradient_descent: boolean\n",
    "        True or false depending if gradient descent should be used when training. If\n",
    "        false then we use batch optimization by least squares.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=.1):\n",
    "        self.param = None\n",
    "        self.learning_rate = ...\n",
    "        self.sigmoid = ...\n",
    "\n",
    "    def _initialize_parameters(self, X):\n",
    "        n_features = np.shape(X)[1]\n",
    "        # Initialize parameters between [-1/sqrt(N), 1/sqrt(N)]\n",
    "        limit = 1 / math.sqrt(n_features)\n",
    "        self.param = np.random.uniform(..., ..., (...,))\n",
    "\n",
    "    def fit(self, X, y, n_iterations=4000):\n",
    "        self._initialize_parameters(X) # Initially the theta parameters are chosen randomly.\n",
    "        # Tune parameters for n iterations\n",
    "        for i in range(n_iterations):\n",
    "            # Make a new prediction\n",
    "            y_pred = ... # Apply the red equation of the logistic model\n",
    "            # Move against the gradient of the loss function with\n",
    "            # respect to the parameters to minimize the loss\n",
    "            self.param -= ... # Tips : Have a look to the blue equation\n",
    "    def predict(self, X):\n",
    "        y_pred = ... # Predict the class of x_i\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca517c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/logistic.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8711d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.1.4 : </b>\n",
    "\n",
    "1. Train your homemade logistic regression model on the train data. \n",
    "\n",
    "2. Display the decision boundaries.\n",
    "\n",
    "3. Evaluate the model on the test data `eval_model` method defined above.\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3851937-69bd-4795-9976-1f3c8c77ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bef269",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.1.5 : </b>\n",
    "\n",
    "In your mind, why are the decision boundaries and score between your model and the sklearn model different ?\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3701f64-a868-441e-95b2-7a26936ade6f",
   "metadata": {},
   "source": [
    "Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315eb90-b868-4d52-acdd-22e858c946f2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.1.6 : </b>\n",
    "\n",
    "Why is it important to normalize your data for logistic regression?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c513daea-7de5-4d4c-9189-93820172ddc9",
   "metadata": {},
   "source": [
    "Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139f2e6-e9f3-4298-9f68-8feb8540ad05",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.1.7 : </b>\n",
    "\n",
    "How can you interpret the model coefficient's ?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e746b8ff-8bc8-429c-8181-371497a6e9e2",
   "metadata": {},
   "source": [
    "Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81eac4",
   "metadata": {},
   "source": [
    "### 1.3.2 A first regression model: Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d7e71",
   "metadata": {},
   "source": [
    "Previously you have seen the logistic regression. Linear regression is its equivalent for classification problems.\n",
    "If you understand logistic regression then this part will be super simple!\n",
    "The simple linear regression model is the basic model simple regression. It has the advantage of being very simple and easily explained. Despite its simplicity, it is widely used, especially in physics, statistics and biology.\n",
    "The prediction for a sample $X^{(i)}$ is defined as :\n",
    "\\begin{equation}\n",
    "S\\left(X^{(i)}\\right)=\\theta_{0}+\\theta_{1} x_{1}+\\theta_{2} x_{2}+\\theta_{3} x_{3}+\\ldots .+\\theta_{n} x_{n} + \\beta = \\sum_{i=0}^{n+1}\\left(\\theta_{i} x_{i}\\right) + \\beta = \\Theta X^{(i)} + \\beta\n",
    "\\end{equation}\n",
    "with $\\theta_i \\in \\mathbf{R}$ for all $i \\in 1,2,\\ldots,n$ and $\\beta \\in \\mathbf{R}$ .\n",
    "As you can see we have introduced a new beta variable. It corresponds to the **bias of the model**. For pedagogical reasons we did not introduce it for the logistic model (we considered the case where $\\beta=0$)\n",
    "\n",
    "\n",
    "\n",
    "In practice if $X$ is the sample matrix, $\\Theta$ the parameters vector and $\\sigma$ the sigmoid function.\n",
    "So, the logistic model is defined as :\n",
    "\n",
    "<span style=\"color:red\"> $$M(X,\\Theta,\\beta) = X \\Theta + \\beta\\mathbf{1} $$</span>\n",
    "where $\\mathbf{1} \\in \\mathbf{R}^N $ is the $N$-dimensional vector where all coordinates are equal to $1$, (with $N$ equal to the number of samples in $X$)\n",
    "\n",
    "We want to find the best $\\theta$ and $\\beta$ such that : \n",
    "    $$L_{MSE}(\\hat{y}-y)= \\|\\hat{Y}-Y\\|^{2} = \\|(X \\Theta+\\beta \\mathbf{1})-Y\\|^{2}$$\n",
    "\n",
    "Thanks to the gradient algorithm, we can iteratively find the best parameters.\n",
    "\n",
    "The partial derivative of the parameters $\\theta$ is given by : \n",
    "<span style=\"color:blue\">\\begin{equation}\n",
    "\\frac{\\partial L_{\\mathrm{CE}}(\\hat{y}, y)}{\\partial \\theta}=[y-\\sigma(\\theta \\cdot x)] x=[y-\\hat{y}] x\n",
    "\\end{equation}</span>\n",
    "\n",
    "The partial derivative of the parameters $\\beta$ is given by : \n",
    "<span style=\"color:blue\">\\begin{equation}\n",
    "\\frac{\\partial L_{\\mathrm{CE}}(\\hat{y}, y)}{\\partial \\beta}=[y-\\sigma(\\theta \\cdot x)] x=[y-\\hat{y}]\n",
    "\\end{equation}</span>\n",
    "<img src=\"./images/gradient_descent_parameter_a.gif\" alt=\"tree\" width=\"600\"/>\n",
    "source:baptiste-monpezat\n",
    "\n",
    "\n",
    "**The gradients are noted in a vector manner to facilitate the transition to the implementation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e005e814",
   "metadata": {},
   "source": [
    "#### Sklearn linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47293ec",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.2.1 : </b>\n",
    "\n",
    "1. Train a sklearn linear regression model on the train data. \n",
    "\n",
    "2. Evaluate the model on the test data using the `eval_model` method defined above.\n",
    "\n",
    "4. Try to get the best score.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5940a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1543c2-fc30-4fb2-86ad-4eb57c3f081e",
   "metadata": {},
   "source": [
    "#### Homemade linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54692bea-bd41-49f7-9e25-a741315cd410",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.2.2 : </b>\n",
    "\n",
    "1. Train your homemade logistic regression model on the train data. \n",
    "2. Evaluate the model on the test data `eval_model` method defined above.\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85758edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomemadeSimpleLinearRegression(AibtMachineLearningModel):\n",
    "    \"\"\" Logistic Regression classifier.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate: float\n",
    "        The step length that will be taken when following the negative gradient during\n",
    "        training.\n",
    "    gradient_descent: boolean\n",
    "        True or false depending if gradient descent should be used when training. If\n",
    "        false then we use batch optimization by least squares.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        self.param = None\n",
    "        self.bias = None\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def _initialize_parameters(self, X):\n",
    "        n_features = np.shape(X)[1]\n",
    "        # Initialize parameters between [-1/sqrt(N), 1/sqrt(N)]\n",
    "        limit = 1 / math.sqrt(n_features)\n",
    "        self.param = ...\n",
    "        self.bias = ...\n",
    "    def fit(self, X, y, n_iterations=4000):\n",
    "        self._initialize_parameters(X) # Initially the theta parameters are chosen randomly.\n",
    "        # Tune parameters for n iterations\n",
    "        for i in range(n_iterations):\n",
    "            # Make a new prediction\n",
    "            y_pred = ...\n",
    "            # Move against the gradient of the loss function with\n",
    "            # respect to the parameters to minimize the loss\n",
    "            self.param -= ...\n",
    "            self.bias -= ...\n",
    "    def predict(self, X):\n",
    "        y_pred = ...\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/linear_regression.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91cd2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80b8b8-c779-4492-9fd8-116434c82c07",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.2.3 : </b>\n",
    "    Imagine that after training a model you obtain the following regression coefficients: \n",
    "\n",
    "$$Y=a+b X_{1}+c X_{2}$$ with the following meaning : \n",
    "$$\\text { house_price }=a+50,000^{*} \\text { square_footage }-20,000^{*} \\text { age }$$\n",
    "\n",
    "    \n",
    "In this case :\n",
    "    \n",
    "1. What is the target that the model estimates ?\n",
    "    \n",
    "2. How does the model estimate this target (According to the training features) ?\n",
    "3. How to interpret a variation of one unit for the coefficient $square\\_footage$ ?\n",
    "4. How to interpret a variation of one unit for the coefficient $age$ ?\n",
    "5. For this model, is the interpretation of coefficient $a$ consistent with reality ?\n",
    "6. **Bonus** : What value should the variable $a$ have for the regression to be consistent with reality? \n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8f77d7-c40d-4a3f-a302-fb1c6da318dd",
   "metadata": {},
   "source": [
    "Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b142b12-22ef-47a3-9fdb-38b76733c351",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.2.4 : </b>\n",
    "\n",
    "Using the previous question, how do you interpret the coefficients in general?Using the previous question, how do you interpret the coefficients in general?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8700c9a5-1f2a-4c8b-b3f0-ee796d215b46",
   "metadata": {},
   "source": [
    "Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda7d5b-a61e-49ba-a9d0-c2c3803c641e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.2.5 : </b>\n",
    "\n",
    "**Bonus** (tough question):\n",
    " \n",
    "Imagine that after training a model you obtain the following logistic regression coefficients: \n",
    "\n",
    "$$p =\\sigma(a+b X_{1}+c X_{2})$$\n",
    "    \n",
    "$$logit(p) = \\sigma^{-1}(a+b X_{1}+c X_{2}) = a+b X_{1}+c X_{2}$$ with $logit(p) = log(\\frac{p}{1-p})$ called the log-odds ratio\n",
    "    $$logit(p)=0.5+0.13 * \\text { study_hours }+0.97 * \\text { passion }$$\n",
    "\n",
    "In this case :\n",
    "    \n",
    "1. What is the target that the model estimates (explain $logit(p)$) ?\n",
    "    \n",
    "2. How does the model estimate this target (According to the training features) ?\n",
    "3. How to interpret a variation of one unit for the coefficient $study\\_hours$ ?\n",
    "4. How to interpret a variation of one unit for the coefficient $passion$ ?\n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e596da7-258f-4722-9d64-f262620940e8",
   "metadata": {},
   "source": [
    "Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747af3d3",
   "metadata": {},
   "source": [
    "### 1.3.3 KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dddd33c",
   "metadata": {},
   "source": [
    "The idea of the KNN model can be summarized in one sentence: **\"tell me who your neighbors are, I'll tell you who you are !\"**\n",
    "This algorithm allows to classify individuals by studying the spatial relationships of the samples.\n",
    "\n",
    "KNN classifier algorithm : \n",
    "\n",
    "**INPUT DATA** :\n",
    "\n",
    "* A set of training data $X_{train}$ with label $y_{train}$.\n",
    "* A distance function $d$.\n",
    "* An integer $K$ (the number of neighbors).\n",
    "* A set of testing data $X_{test}$ (unlabeled).\n",
    "\n",
    "**BEGIN ALGORITHM** : \n",
    "\n",
    "For each unlabelled data $x_{test}$ in $X_{test}$:\n",
    "\n",
    "* Compute all distances of this observation $x_{test}$ from the other observations in the dataset $X_{train}$.\n",
    "* Select the $K$ observations of the dataset $X_{train}$ that are closest to $x_{test}$  by using the distance function $d$.\n",
    "* Predict the label $y_{test}$ of $x_{test}$ using a decision rule on the k-neighbors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a3530",
   "metadata": {},
   "source": [
    "#### Sklearn KNN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d493a573-40f3-4694-a0bb-50e9c9db1a8c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.3.1 : </b>\n",
    "\n",
    "1. Train a sklearn KNN model on the train data. \n",
    "\n",
    "2. Evaluate the model on the test data using the `eval_model` method defined above.\n",
    "\n",
    "4. Try to get the best score.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af559ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf6324-06c2-4654-941a-6d3fbe6a2399",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.3.2 : </b>\n",
    "\n",
    "1. Train your homemade Knn on the train data. \n",
    "\n",
    "2. Display the decision boundaries.\n",
    "\n",
    "3. Evaluate the model on the test data `eval_model` method defined above.\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f781b52",
   "metadata": {},
   "source": [
    "#### Homemade KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b69a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1, x2):\n",
    "        \"\"\" Compute the distance between two vectors \"\"\"\n",
    "        return ...\n",
    "\n",
    "class HomemadeKNeighborsClassifier(AibtMachineLearningModel):\n",
    "    \"\"\" K Nearest Neighbors classifier.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    k: int\n",
    "        The number of closest neighbors that will determine the class of the \n",
    "        sample that we wish to predict.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_neighbors=5):\n",
    "        self.n_neighbors = n_neighbors\n",
    "    \n",
    "    def decision_rule(self, neighbor_labels):\n",
    "        \"\"\" Return the most common class among the neighbor samples \"\"\"\n",
    "        return ...\n",
    "        \n",
    "    \n",
    "    def fit(self,X_train,y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_pred = np.empty(X_test.shape[0])\n",
    "        ...\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/KNN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9966bca-e67f-4598-9ad7-e421c59ffd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d6d0ee-3b95-4f53-8820-3db8862e36cb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.3.3 : </b>\n",
    "\n",
    "Why is it important to normalize your data for the Knn?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82664d34-975b-4d55-83b7-15f0c882046d",
   "metadata": {},
   "source": [
    "Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ee50c1-2b97-41e0-a988-7d75e12480c2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.3.4 : </b>\n",
    "\n",
    "Can you explain the impact of changing the number of neighbours?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65717b95-0839-490f-b3f5-3a4b916f49b5",
   "metadata": {},
   "source": [
    "Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c27b4e",
   "metadata": {},
   "source": [
    "## 1.3.4 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aa3f6a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "It would be best to code the decision tree yourself in this section, as this is cumbersome for an introductory course. However, it is necessary to understand the algorithm and the code below to answer the exercises correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58180197-f81c-4acf-aac1-d75b6cdc195e",
   "metadata": {},
   "source": [
    "**Using** `Scikit learn`: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b0a6e-476e-4763-83e2-bc87dd314c46",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.4.1 : </b>\n",
    "\n",
    "1. Train a sklearn decision tree model on the train data. \n",
    "\n",
    "2. Display the decision boundaries.\n",
    "\n",
    "3. Evaluate the model on the test data using the `eval_model` method defined above.\n",
    "\n",
    "4. Try to get the best score and the most beautiful decision boundaries.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f5aadd-1440-4cff-9eee-2481f67ff60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60fcc66-752d-47ce-af8c-bc24e651c71e",
   "metadata": {},
   "source": [
    "Let's visualize the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc3c19b-d8f2-4c37-9bc1-4a7f0b7d8ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "plt.figure(figsize=(16, 12), dpi=80)\n",
    "info_tree = tree.plot_tree(model_cart, feature_names= [\"x_axis\",\" y_axis\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d9fec4-79c0-42b0-9f6b-ae926713b4a1",
   "metadata": {},
   "source": [
    "**From scratch**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a548cf8",
   "metadata": {},
   "source": [
    "The tree data structure is composed of nodes and leaves.\n",
    "- Leaves contain one value\n",
    "- A node contains two values called left and right.\n",
    "These values can be a node or a leaf.\n",
    "\n",
    "\n",
    "<img src=\"./images/tree_data_structure.png\" alt=\"tree\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2488b2a-2645-4b00-9fd7-bccca9842e23",
   "metadata": {},
   "source": [
    "**From scratch**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3181dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, left, right, rule):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        # Rule is a tuple : {feature, threshold}\n",
    "        self.feature = rule[0]\n",
    "        self.threshold = rule[1]\n",
    "\n",
    "# If we are modern we can also use a dataclass\n",
    "class Leaf:\n",
    "    def __init__(self, value):\n",
    "        \"\"\"\n",
    "        `value` is an array of class probabilities if classifier is True, else\n",
    "        the mean of the region\n",
    "        \"\"\"\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b83442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_criterion(y):\n",
    "    \"\"\"\n",
    "    Mean squared error for decision tree (ie., mean) predictions\n",
    "    \"\"\"\n",
    "    return np.mean((y - np.mean(y)) ** 2)\n",
    "\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Entropy of a label sequence.\n",
    "    \"\"\"\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / np.sum(hist)\n",
    "    return - np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "\n",
    "def gini(y):\n",
    "    \"\"\"\n",
    "    Gini impurity (local entropy) of a label sequence.\n",
    "    \"\"\"\n",
    "    hist = np.bincount(y)\n",
    "    N = np.sum(hist)\n",
    "    return 1 - sum([(i / N) ** 2 for i in hist])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d2d2db-6d69-4c9b-98b5-739927c1f3ec",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.4.1 : </b>\n",
    "\n",
    "Explain with your words the entropy and the gini coefficient. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dfb7f9-b748-4c79-ae89-cdf826da39ca",
   "metadata": {},
   "source": [
    "Write your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a202e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(AibtMachineLearningModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier: bool =True,\n",
    "        max_depth: int =None,\n",
    "        n_feats: int=None,\n",
    "        criterion: str =\"entropy\",\n",
    "        seed: int =None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A decision tree model for regression and classification problems.\n",
    "        Parameters\n",
    "        ----------\n",
    "        classifier : bool\n",
    "            Whether to treat target values as categorical (classifier =\n",
    "            True) or continuous (classifier = False). Default is True.\n",
    "        max_depth: int or None\n",
    "            The depth at which to stop growing the tree. If None, grow the tree\n",
    "            until all leaves are pure. Default is None.\n",
    "        n_feats : int\n",
    "            Specifies the number of features to sample on each split. If None,\n",
    "            use all features on each split. Default is None.\n",
    "        criterion : {'mse', 'entropy', 'gini'}\n",
    "            The error criterion to use when calculating splits. When\n",
    "            `classifier` is False, valid entries are {'mse'}. When `classifier`\n",
    "            is True, valid entries are {'entropy', 'gini'}. Default is\n",
    "            'entropy'.\n",
    "        seed : int or None\n",
    "            Seed for the random number generator. Default is None.\n",
    "        \"\"\"\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.depth = 0\n",
    "        self.root = None\n",
    "\n",
    "        self.n_feats = n_feats\n",
    "        self.criterion = criterion\n",
    "        self.classifier = classifier\n",
    "        self.max_depth = max_depth if max_depth else np.inf\n",
    "\n",
    "        if not classifier and criterion in [\"gini\", \"entropy\"]:\n",
    "            raise ValueError(\n",
    "                \"{} is a valid criterion only when classifier = True.\".format(criterion)\n",
    "            )\n",
    "        if classifier and criterion == \"mse\":\n",
    "            raise ValueError(\"`mse` is a valid criterion only when classifier = False.\")\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fit a binary decision tree to a dataset.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n",
    "            The training data of `N` examples, each with `M` features\n",
    "        Y : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
    "            An array of integer class labels for each example in `X` if\n",
    "            self.classifier = True, otherwise the set of target values for\n",
    "            each example in `X`.\n",
    "        \"\"\"\n",
    "        self.n_classes = max(Y) + 1 if self.classifier else None # We determine the number of classes\n",
    "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
    "        self.root = self._grow(X, Y) # Tree construction\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained decision tree to classify or predict the examples in `X`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n",
    "            The training data of `N` examples, each with `M` features\n",
    "        Returns\n",
    "        -------\n",
    "        preds : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
    "            The integer class labels predicted for each example in `X` if\n",
    "            self.classifier = True, otherwise the predicted target values.\n",
    "        \"\"\"\n",
    "        return np.array([self._traverse(x, self.root) for x in X])\n",
    "\n",
    "    def predict_class_probs(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained decision tree to return the class probabilities for the\n",
    "        examples in `X`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n",
    "            The training data of `N` examples, each with `M` features\n",
    "        Returns\n",
    "        -------\n",
    "        preds : :py:class:`ndarray <numpy.ndarray>` of shape `(N, n_classes)`\n",
    "            The class probabilities predicted for each example in `X`.\n",
    "        \"\"\"\n",
    "        assert self.classifier, \"`predict_class_probs` undefined for classifier = False\"\n",
    "        return np.array([self._traverse(x, self.root, prob=True) for x in X])\n",
    "\n",
    "    def _grow(self, X, Y, cur_depth=0):\n",
    "        # if all labels are the same, return a leaf because the split is \"pure\"\n",
    "        if len(set(Y)) == 1:\n",
    "            if self.classifier:\n",
    "                prob = np.zeros(self.n_classes)\n",
    "                prob[Y[0]] = 1.0\n",
    "            return Leaf(prob) if self.classifier else Leaf(Y[0])\n",
    "\n",
    "        # if we have reached max_depth, return a leaf\n",
    "        if cur_depth >= self.max_depth:\n",
    "            v = np.mean(Y, axis=0)\n",
    "            if self.classifier:\n",
    "                v = np.bincount(Y, minlength=self.n_classes) / len(Y)\n",
    "            # Initialize the leaf with the mean of Y (for regression),\n",
    "            # or the proportion of each class on the leaf (for classification)\n",
    "            return Leaf(v)\n",
    "\n",
    "        cur_depth += 1\n",
    "        self.depth = max(self.depth, cur_depth)\n",
    "\n",
    "        N, M = X.shape\n",
    "        feat_idxs = np.random.choice(M, self.n_feats, replace=False)\n",
    "\n",
    "        # greedily select the best split according to `criterion`\n",
    "        feat, thresh = self._segment(X, Y, feat_idxs)\n",
    "        l = np.argwhere(X[:, feat] <= thresh).flatten()\n",
    "        r = np.argwhere(X[:, feat] > thresh).flatten()\n",
    "\n",
    "        # grow the children that result from the split\n",
    "        # recursive call for building the tree\n",
    "        left = self._grow(X[l, :], Y[l], cur_depth)\n",
    "        right = self._grow(X[r, :], Y[r], cur_depth)\n",
    "        return Node(left, right, (feat, thresh))\n",
    "\n",
    "    def _segment(self, X, Y, feat_idxs):\n",
    "        \"\"\"\n",
    "        Find the optimal split rule (feature index and split threshold) for the\n",
    "        data according to `self.criterion`.\n",
    "        \"\"\"\n",
    "        best_gain = -np.inf # minimum value possible \n",
    "        split_idx, split_thresh = None, None # initalize best split \n",
    "        for i in feat_idxs:\n",
    "            vals = X[:, i]\n",
    "            levels = np.unique(vals)\n",
    "            thresholds = (levels[:-1] + levels[1:]) / 2 if len(levels) > 1 else levels\n",
    "            gains = np.array([self._impurity_gain(Y, t, vals) for t in thresholds])\n",
    "\n",
    "            if gains.max() > best_gain:\n",
    "                split_idx = i\n",
    "                best_gain = gains.max()\n",
    "                split_thresh = thresholds[gains.argmax()]\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _impurity_gain(self, Y, split_thresh, feat_values):\n",
    "        \"\"\"\n",
    "        Compute the impurity gain associated with a given split.\n",
    "        IG(split) = loss(parent) - weighted_avg[loss(left_child), loss(right_child)]\n",
    "        \"\"\"\n",
    "        if self.criterion == \"entropy\":\n",
    "            loss = entropy\n",
    "        elif self.criterion == \"gini\":\n",
    "            loss = gini\n",
    "        elif self.criterion == \"mse\":\n",
    "            loss = mse_criterion\n",
    "\n",
    "        parent_loss = loss(Y)\n",
    "\n",
    "        # generate split\n",
    "        left = np.argwhere(feat_values <= split_thresh).flatten()\n",
    "        right = np.argwhere(feat_values > split_thresh).flatten()\n",
    "\n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            return 0\n",
    "\n",
    "        # compute the weighted avg. of the loss for the children\n",
    "        n = len(Y)\n",
    "        n_l, n_r = len(left), len(right)\n",
    "        e_l, e_r = loss(Y[left]), loss(Y[right])\n",
    "        child_loss = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        # impurity gain is difference in loss before vs. after split\n",
    "        ig = parent_loss - child_loss\n",
    "        return ig\n",
    "\n",
    "    def _traverse(self, X, node, prob=False):\n",
    "        if isinstance(node, Leaf):\n",
    "            if self.classifier:\n",
    "                return node.value if prob else node.value.argmax()\n",
    "            return node.value\n",
    "        if X[node.feature] <= node.threshold:\n",
    "            return self._traverse(X, node.left, prob)\n",
    "        return self._traverse(X, node.right, prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a743c-b17c-4b84-92c5-7ef51075267c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.4.2 : </b>\n",
    "\n",
    "1. Train your homemade decision tree model on the train data. \n",
    "\n",
    "2. Display the decision boundaries.\n",
    "\n",
    "3. Evaluate the model on the test data `eval_model` method defined above.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3474eea1-7d6d-409e-9318-e343e7e5c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3793077-eea7-437c-9af4-0926096a5d5a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.4.3 : </b>\n",
    "\n",
    "After carefully reading the code above, you will explain in your own words how the decision tree works; How to build it (training); How we predict with it.\n",
    "\n",
    "You will make three paragraphs; feel free to add figures to support your explanations. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce28379-5b49-40c7-981c-dba976f6b156",
   "metadata": {},
   "source": [
    "Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d7c95-a194-496f-89c2-ec9849a72d9c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.4.4 : </b>\n",
    "\n",
    "Why  data scaling is unnecessary for the decision tree?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d3d80-b127-4461-a41e-f22f9ef3932b",
   "metadata": {},
   "source": [
    "Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c4216-0910-4896-abd4-b4a647c0674f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.4.5 : </b>\n",
    "\n",
    "How to interpret a decision tree?\n",
    "What criteria will you use? \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c07d42-70c0-40d0-b237-be2e299a6780",
   "metadata": {},
   "source": [
    "Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c783a451-068c-4b09-9e02-949c02597649",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> **Bonus** Exercise 1.3.4.6 : </b>\n",
    "\n",
    "**This question is challenging**.\n",
    "\n",
    "Suppose you are using a decision tree on a time series problem.\n",
    "The trend is positive and not null (i.e., the target value is non-stationary and will increase over time). \n",
    "\n",
    "Is the decision tree efficient for this use case? Why? \n",
    "\n",
    "What pre-processing of the data do you have to do to use decision trees for this use case?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6de673-4180-46fc-87c1-07f29ad85ab2",
   "metadata": {},
   "source": [
    "Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46822d8a-708d-4a60-98e1-4dd5d72c9dfd",
   "metadata": {},
   "source": [
    "## 1.3.5 Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2685ddb1-447a-45dd-8e00-945202c9e22b",
   "metadata": {},
   "source": [
    "Ensembles are a crucial notion in machine learning. \n",
    "Generally, they are the algorithms with the strongest predictive power (most of the wins on Kaggle are based on Ensemble). \n",
    "\n",
    "There are three types of Ensemble: \n",
    "- **Bagging** (Bootstrap aggregating): Train n estimators and average the predictions.\n",
    "There are many variations, such as training each estimator with a random fraction of the data and a random number of features to have the most different estimators.\n",
    "The more heterogeneous the estimators are, the more efficient the bagging is.\n",
    "- **Boosting**: It is a succession of weak estimators. The objective is to correct the error of the previous estimator. There are many variations, the most famous of which is gradient boosting. If you want to know more, We invite you to read this fantastic [article](https://explained.ai/gradient-boosting/).\n",
    "Boosting, significantly gradient boosting, is the most popular and efficient method and is a must-have for a data scientist.\n",
    "- **Stacking**: You train several estimators and learn a meta-estimator on top to optimally combine your estimators. Stacking is the least used ensemble method due to its complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e07382-ff66-42a9-8032-7db2e368fd1a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.5.1 : </b>\n",
    "\n",
    "1. Code your bagging.\n",
    "    \n",
    "2.Take four estimators seen before and use the bagging.\n",
    "    \n",
    "3. Compare the score between the bagging and the models took independently.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0c4660-0e7a-447e-aabf-cb62024215f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggingModel(AibtMachineLearningModel):\n",
    "    def __init__(self, list_model: List[MachineLearningModel]):\n",
    "        ...\n",
    "    \n",
    "    def fit(self, X:np.ndarray, y: np.ndarray):\n",
    "        ...\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8c112d-2bdf-448a-9ebc-ce8a5e63daa3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.5.2 : </b>\n",
    "\n",
    "1. Train your homemade bagging on the train data. \n",
    "\n",
    "2. Display the decision boundaries.\n",
    "\n",
    "3. Evaluate the model on the test data `eval_model` method defined above.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7748cb85-aea2-43c0-a1b3-00bacde2d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97fd738-967f-4ffd-bb74-784857d5ee6e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> **Bonus** Exercise 1.3.5.3 : </b>\n",
    "  \n",
    "\n",
    "1. Code the advanced version of the bagging.Each estimator will learn on a random subset of features and a random fraction of the observations. Also, add these parameters to the class constructor. \n",
    "\n",
    "2. Test your \"advanced\" bagging on the classification dataset and measure its performance. \n",
    "\n",
    "\n",
    "**Beware** that the \"advanced\" bagging for toy datasets may not improve the performance because the use case is too \"simple.\" \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e029ee89-1721-4d04-88aa-ae9ad27f2868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d6d6ed",
   "metadata": {},
   "source": [
    "There are several versions of the adaboost algorithm. Here is a simple but sub-optimal one. The implementation is taken from the excellent article geoffruddock, of course we invite you to consult the code after trying to implement this model by yourself.\n",
    "\n",
    "The idea of adaboost is to train successively $T$ sub models. The goal is to give a weight $\\alpha_t$ to each of the $h_{t}$ models according to their performance.  As the iterations progress, the weighting of the samples $w_i$ will be modified to allow the next model to specialize (to give more importance) to the samples poorly classified by the all previous models.\n",
    "\n",
    "\\begin{array}{|l|l|}\n",
    "\\hline {\\text { Variable }} & \\text { Math } \\\\\n",
    "\\hline \\text { sample_weights with shape: }(T, \\mathrm{n}) & w_{i}^{(t)} \\\\\n",
    "\\hline \\text { stumps with shape: (T, ) } & h_{t}(x) \\\\\n",
    "\\hline \\text { stump_weights with shape (T, ) } & \\alpha_{t} \\\\\n",
    "\\hline \\text { errors with shape: (T, ) } & \\epsilon_{t} \\\\\n",
    "\\hline \\text { clf.predict (X) } & H_{t}(x) \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "1. For each weak learners $h_{t}(x)$ compute $\\epsilon_{t}=\\sum_{i=1}^{n} \\mathbf{1}\\left[h_{t}\\left(x_{i}\\right) \\neq y_{i}\\right] w_{i}^{t}$.\n",
    "2. Set a weight for our weak learner based on its accuracy: $\\alpha_{t}=\\frac{1}{2} \\ln \\left(\\frac{1-\\epsilon_{t}}{\\epsilon_{t}}\\right)$\n",
    "3. Increase weights of misclassified observations: $w_{i}^{(t+1)}=w_{i}^{(t)} \\cdot e^{-\\alpha_{t} y_{i} h_{t}\\left(x_{i}\\right)}$. Note that $y_{i} h_{t}\\left(x_{i}\\right)$ will evaluate to $+1$ when hypothesis agrees with label, and $-1$ when it does not agree.\n",
    "4. Renormalize weights, so that $\\sum_{i=1}^{n} w_{i}^{(t+1)}=1$.\n",
    "\n",
    "The final prediction is given by : \n",
    "$$H_{t}(x)=\\operatorname{sign}\\left(\\sum_{t=1}^{T} \\alpha_{t} h_{t}(x)\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a62551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class AdaBoost(AibtMachineLearningModel):\n",
    "    \"\"\" AdaBoost ensemble classifier from scratch \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stumps = None\n",
    "        self.stump_weights = None\n",
    "        self.sample_weights = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, iters: int):\n",
    "        \"\"\" Fit the model using training data \"\"\"\n",
    "        n = X.shape[0]\n",
    "\n",
    "        # init numpy arrays\n",
    "        self.sample_weights = np.zeros(shape=(iters, n))\n",
    "        self.stumps = np.zeros(shape=iters, dtype=object)\n",
    "        self.stump_weights = np.zeros(shape=iters)\n",
    "        self.errors = np.zeros(shape=iters)\n",
    "\n",
    "        # initialize weights uniformly\n",
    "        self.sample_weights[0] = np.ones(shape=n) / n\n",
    "\n",
    "        for t in range(iters):\n",
    "            # fit  weak learner\n",
    "            curr_sample_weights = ...\n",
    "            stump = DecisionTreeClassifier(max_depth=1, max_leaf_nodes=2)\n",
    "            stump = stump.fit(X, y, sample_weight=curr_sample_weights)\n",
    "\n",
    "            # calculate error and stump weight from weak learner prediction\n",
    "            stump_pred = ...\n",
    "            err = ... # 1. of the algorithm\n",
    "            stump_weight = ... # 2. of the algorithm\n",
    "\n",
    "            # update sample weights\n",
    "            new_sample_weights = ... # 3. of the algorithm \n",
    "            # weight normalization \n",
    "            new_sample_weights /= ... # 4. of the algorithm \n",
    "\n",
    "            # If not final iteration, update sample weights for t+1\n",
    "            if t+1 < iters:\n",
    "                self.sample_weights[t+1] = new_sample_weights\n",
    "\n",
    "            # save results of iteration\n",
    "            self.stumps[t] = stump\n",
    "            self.stump_weights[t] = stump_weight\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        \"\"\" Make predictions using already fitted model \"\"\"\n",
    "        y_pred = ...\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49e0a4-96fb-4522-a3de-d231aba7734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/adaboost.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e2464-d975-4337-b7e6-6b67b3669839",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.5.4 : </b>\n",
    "\n",
    "1. Train your adaboost on the train data. \n",
    "\n",
    "2. Display the decision boundaries.\n",
    "\n",
    "3. Evaluate the model on the test data `eval_model` method defined above.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819df95b-5574-4e99-9498-27da8743b49a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 666 : </b>\n",
    "    \n",
    "\n",
    "Wow! Have you finished the exercices ? \n",
    "    \n",
    "Stromae say: \"Quand c'est fini il y en a encore !\" ... Alors on code !\n",
    "    \n",
    "\n",
    "This is a **bonus**: \n",
    "\n",
    "You can implement it yourself:\n",
    "\n",
    "- A naive bayesian\n",
    "- A linear regression with an l2 penalty (ridge regression)\n",
    "- A linear regression with an l1 penalty (lasso regression)\n",
    "_ A linear regression with an elasticnet\n",
    "- A linear or nonlinear SVM using kernel functions\n",
    "- A gradient boosting\n",
    "- A stacking\n",
    "\n",
    "\n",
    "We are aware that this is difficult, and do not hesitate to contact us by mail.\n",
    "\n",
    "Good luck !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba61abff-3033-4e4f-9a3f-127b296e166e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
